# RAG: Retrieval Augmented Generation

RAG kombiniert AI mit deinen eigenen Daten für präzise, kontextbezogene Antworten. Diese Technik ist essentiell für Enterprise AI-Anwendungen.

## Was ist RAG?

**Problem**: LLMs haben begrenztes Wissen
- Training-Cutoff-Datum
- Keine firmenspezifischen Daten
- Keine Echtzeit-Informationen

**Lösung**: RAG
1. Speichere deine Dokumente in einer Vektor-Datenbank
2. Suche relevante Infos basierend auf der User-Query
3. Füge gefundene Infos zum Prompt hinzu
4. LLM antwortet mit diesem Kontext

## Komponenten eines RAG-Systems

### 1. Embeddings

Texte werden in mathematische Vektoren umgewandelt:

```javascript
import OpenAI from 'openai';

const openai = new OpenAI();

const embedding = await openai.embeddings.create({
  model: "text-embedding-3-small",
  input: "Dein Text hier",
});

console.log(embedding.data[0].embedding);
// [0.002, -0.015, 0.042, ...] (1536 Dimensionen)
```

### 2. Vektor-Datenbank

Populäre Optionen:
- **Pinecone**: Managed, einfach
- **Weaviate**: Open Source, flexibel
- **Qdrant**: Schnell, Rust-basiert
- **ChromaDB**: Lightweight, lokal

Beispiel mit Pinecone:

```javascript
import { Pinecone } from '@pinecone-database/pinecone';

const pc = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const index = pc.index('knowledge-base');

// Dokument speichern
await index.upsert([{
  id: 'doc1',
  values: embedding,
  metadata: { text: 'Original text...', source: 'docs/guide.md' }
}]);
```

### 3. Retrieval

Finde ähnliche Dokumente:

```javascript
async function retrieve(query, topK = 3) {
  // Query embedden
  const queryEmbedding = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });

  // Ähnlichkeitssuche
  const results = await index.query({
    vector: queryEmbedding.data[0].embedding,
    topK,
    includeMetadata: true,
  });

  return results.matches.map(m => m.metadata.text);
}
```

### 4. Augmented Prompt

Kombiniere Kontext mit User-Query:

```javascript
async function ragQuery(userQuery) {
  // Schritt 1: Relevante Dokumente finden
  const context = await retrieve(userQuery);

  // Schritt 2: Prompt mit Kontext erstellen
  const prompt = `
Beantworte die Frage basierend auf folgendem Kontext:

Kontext:
${context.join('\n\n')}

Frage: ${userQuery}

Antwort:
`;

  // Schritt 3: LLM aufrufen
  const completion = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }],
  });

  return completion.choices[0].message.content;
}
```

## Chunking-Strategien

Große Dokumente müssen in Chunks aufgeteilt werden:

### Fixed-Size Chunking
```javascript
function chunkText(text, chunkSize = 512, overlap = 50) {
  const chunks = [];
  for (let i = 0; i < text.length; i += chunkSize - overlap) {
    chunks.push(text.slice(i, i + chunkSize));
  }
  return chunks;
}
```

### Semantic Chunking
- Nach Absätzen
- Nach Überschriften
- Nach Sinnabschnitten

## Optimierungs-Tipps

### 1. Hybrid Search

Kombiniere Vektor- und Keyword-Suche:

```javascript
// Vektor-Suche für semantische Ähnlichkeit
const vectorResults = await vectorSearch(query);

// Keyword-Suche für exakte Matches
const keywordResults = await keywordSearch(query);

// Kombiniere und ranke
const combined = mergeAndRank(vectorResults, keywordResults);
```

### 2. Re-Ranking

Sortiere Ergebnisse nach Relevanz:

```javascript
import Cohere from 'cohere-ai';

const cohere = new Cohere({ apiKey: process.env.COHERE_API_KEY });

const reranked = await cohere.rerank({
  model: 'rerank-english-v2.0',
  query: userQuery,
  documents: retrievedDocs,
  topN: 3,
});
```

### 3. Metadata-Filtering

Filtere nach Metadaten:

```javascript
const results = await index.query({
  vector: queryEmbedding,
  filter: {
    department: 'engineering',
    date: { $gte: '2024-01-01' },
  },
  topK: 5,
});
```

## Evaluation

Messe RAG-Qualität:

### Metriken
- **Relevance**: Sind gefundene Dokumente relevant?
- **Faithfulness**: Bleibt die Antwort beim Kontext?
- **Answer Quality**: Ist die Antwort hilfreich?

### Tools
- **RAGAS**: RAG Assessment Framework
- **LangSmith**: Monitoring und Tracing
- **Phoenix**: Open Source Observability

## Produktions-Architektur

```
User Query
    ↓
Query Router (Intent Detection)
    ↓
Retrieval Pipeline
    ├─ Embedding
    ├─ Vector Search
    ├─ Hybrid Search
    └─ Re-Ranking
    ↓
Context Assembly
    ↓
LLM Generation
    ↓
Response + Citations
```

## Nächste Schritte

In der nächsten Lektion bauen wir AI-Agents, die RAG und andere Tools kombinieren.
